import random
import torch
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import HfApi, get_token, login, snapshot_download
from tqdm import tqdm

# ç¢ºä¿åœ¨ Inference Mode ä¸‹åŸ·è¡Œï¼Œç¯€çœè¨˜æ†¶é«”
torch.inference_mode()

# 1. è¨­å®šæ¨¡å‹èˆ‡åƒæ•¸
MODEL_ID = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
SAVE_PATH = "D:\Ai\deepseek-r1-1.5B_refusal"  # ä¿®æ”¹å¾Œæ¨¡å‹çš„å„²å­˜è·¯å¾‘
REPO_ID = "huiink/deepseek-r1-1.5B-abliterated" # ä½ çš„ HF å¸³è™Ÿ/æ¨¡å‹åç¨±

# ğŸ’¡ å¼·çƒˆå»ºè­°ä½¿ç”¨ bfloat16ï¼šé€™æ˜¯å¤§æ¨¡å‹åŸç”Ÿè¨“ç·´çš„ç²¾åº¦ï¼Œèƒ½é¿å…ç²¾åº¦è½‰æ›å¸¶ä¾†çš„æ•ˆèƒ½é€€åŒ–èˆ‡è¨˜æ†¶é«”æµªè²»
print(f"Loading model: {MODEL_ID} in bfloat16...")

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    trust_remote_code=True,
    dtype=torch.bfloat16, 
    device_map="cuda",
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)

# settings:
instructions = 64
# è¨­å®šæå–å±¤
layer_idx = int(len(model.model.layers) * 0.6) 
pos = -1

print("Instruction count: " + str(instructions))
print("Extraction Layer index: " + str(layer_idx))

# 2. è®€å–è³‡æ–™èˆ‡è¨ˆç®— Refusal Vector
with open("harmful.txt", "r", encoding="utf-8") as f:
    harmful = f.readlines()

with open("harmless.txt", "r", encoding="utf-8") as f:
    harmless = f.readlines()

harmful_instructions = random.sample(harmful, instructions)
harmless_instructions = random.sample(harmless, instructions)

harmful_toks = [
    tokenizer.apply_chat_template(conversation=[{"role": "user", "content": insn}],
        add_generation_prompt=True,
        return_tensors="pt") for insn in harmful_instructions]
harmless_toks = [
    tokenizer.apply_chat_template(conversation=[{"role": "user", "content": insn}],
        add_generation_prompt=True,
        return_tensors="pt") for insn in harmless_instructions]

max_its = instructions * 2
bar = tqdm(total=max_its, desc="Generating Activations")
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

def generate(toks):
    bar.update(n=1)
    
    # æª¢æŸ¥è¼¸å…¥å‹åˆ¥ä¸¦æå– input_ids
    if isinstance(toks, dict) or hasattr(toks, 'input_ids'):
        input_ids = toks['input_ids']
    else:
        input_ids = toks

    # ç§»å‹•åˆ° GPU/CPU
    input_ids = input_ids.to(model.device)
    
    # æ‰‹å‹•å»ºç«‹ Attention Mask
    attention_mask = torch.ones(input_ids.shape, device=model.device, dtype=torch.long)
    
    # åŸ·è¡Œç”Ÿæˆ
    return model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        use_cache=False,
        max_new_tokens=1,
        return_dict_in_generate=True,
        output_hidden_states=True,
        pad_token_id=tokenizer.pad_token_id
    )

harmful_outputs = [generate(toks) for toks in harmful_toks]
harmless_outputs = [generate(toks) for toks in harmless_toks]

bar.close()

harmful_hidden = [output.hidden_states[0][layer_idx][:, pos, :] for output in harmful_outputs]
harmless_hidden = [output.hidden_states[0][layer_idx][:, pos, :] for output in harmless_outputs]

harmful_mean = torch.stack(harmful_hidden).mean(dim=0)
harmless_mean = torch.stack(harmless_hidden).mean(dim=0)

refusal_dir = harmful_mean - harmless_mean
refusal_dir = refusal_dir / refusal_dir.norm()

print("\nRefusal Vector Calculated.")
print(f"Vector norm: {refusal_dir.norm().item()}")

# ---------------------------------------------------------
# 3. åŸ·è¡Œæ¬Šé‡æ­£äº¤åŒ– (Weight Orthogonalization)
# ---------------------------------------------------------

print(f"\nStarting Ablation (Orthogonalization)...")
print(f"Targeting layers from {layer_idx} to {len(model.model.layers)}")

def orthogonalize_weight(weight, direction):
    # ä¿å­˜åŸå§‹å½¢ç‹€ï¼ˆé‡è¦ï¼é˜²æ­¢è®Šæˆ 4Dï¼‰
    orig_shape = weight.shape
    
    # å¼·åˆ¶è½‰æˆ 2D è™•ç†
    if weight.ndim > 2:
        weight_2d = weight.view(orig_shape[0], -1)
    else:
        weight_2d = weight
    
    direction = direction.view(-1).to(weight_2d.device, dtype=weight_2d.dtype)
    
    v = direction.unsqueeze(1)
    P = torch.matmul(v, v.T)
    I = torch.eye(P.shape[0], device=weight_2d.device, dtype=weight_2d.dtype)
    Q = I - P
    
    with torch.no_grad():
        new_weight = torch.matmul(weight_2d, Q)
        weight.copy_(new_weight.view(orig_shape))  # é‚„åŸåŸå§‹å½¢ç‹€

target_layers = model.model.layers[layer_idx:] 

for i, layer in enumerate(tqdm(target_layers, desc="Ablating Layers")):
    modules_to_modify = []
    
    # Attention Projections
    if hasattr(layer.self_attn, "q_proj"): modules_to_modify.append(layer.self_attn.q_proj)
    if hasattr(layer.self_attn, "k_proj"): modules_to_modify.append(layer.self_attn.k_proj)
    if hasattr(layer.self_attn, "v_proj"): modules_to_modify.append(layer.self_attn.v_proj)
    
    # # MLP Projections
    if hasattr(layer.mlp, "gate_proj"): modules_to_modify.append(layer.mlp.gate_proj)
    if hasattr(layer.mlp, "up_proj"): modules_to_modify.append(layer.mlp.up_proj)
    
    # èˆŠæ¶æ§‹é é˜²
    if hasattr(layer.self_attn, "query_key_value"): modules_to_modify.append(layer.self_attn.query_key_value)
    if hasattr(layer.mlp, "dense_h_to_4h"): modules_to_modify.append(layer.mlp.dense_h_to_4h)

    for module in modules_to_modify:
        orthogonalize_weight(module.weight, refusal_dir)

print("\nAblation Complete.")

# ---------------------------------------------------------
# 4. å„²å­˜æ¨¡å‹ä¸¦å®Œç¾é‚„åŸåŸå»  Config
# ---------------------------------------------------------
print(f"\n[1/3] ä¸‹è¼‰åŸå§‹ repo çš„æ‰€æœ‰éæ¬Šé‡æª”æ¡ˆï¼ˆconfigã€modeling_*.pyã€tokenizerï¼‰...")

os.makedirs(SAVE_PATH, exist_ok=True)

snapshot_download(
    repo_id=MODEL_ID,
    local_dir=SAVE_PATH,
    local_dir_use_symlinks=False,
    ignore_patterns=[
        "*.safetensors", "*.bin", "*.pt", "*.msgpack", 
        "optimizer*", "rng_state_*", "trainer_state.json", "*.h5"
    ]
)

print(f"[2/3] å„²å­˜ä¿®æ”¹å¾Œçš„æ¬Šé‡ (safetensors)...")
model.save_pretrained(
    SAVE_PATH, 
    safe_serialization=True,      # å¼·çƒˆå»ºè­°ç”¨ safetensorsï¼ˆæ›´å®‰å…¨ã€æ›´å¿«ï¼‰
    max_shard_size="4GB"        # 1.5B æ¨¡å‹å…¶å¯¦ä¸ç”¨åˆ†ç‰‡ï¼Œå¯ä»¥è¨»è§£æ‰æ›´ä¹¾æ·¨
)

print(f"[3/3] é‡æ–°å„²å­˜ tokenizerï¼ˆç¢ºä¿ chat_template æ­£ç¢ºï¼‰...")
tokenizer.save_pretrained(SAVE_PATH)

print(f"\n æ¨¡å‹å·²å®Œç¾å„²å­˜åˆ° {SAVE_PATH}")
print("   - æ¬Šé‡ï¼šå·² ablateï¼ˆä¿®æ”¹éï¼‰")
print("   - config / modeling_*.py / configuration_*.pyï¼šåŸå§‹å®˜æ–¹")
print("   - tokenizerï¼ˆå« chat_templateï¼‰ï¼šæœ€æ–°ç‹€æ…‹")
# ---------------------------------------------------------
# 5. ä¸Šå‚³è‡³ Hugging Face (Optional)
# ---------------------------------------------------------
upload_choice = input("\nDo you want to upload to Hugging Face now? (y/n): ")

if upload_choice.lower() == 'y':
    token = get_token()
    if token is None:
        print("Token not found. Initiating login...")
        login()
        token = get_token()

    if token:
        try:
            api = HfApi()
            print(f"Creating repo: {REPO_ID} ...")
            api.create_repo(repo_id=REPO_ID, exist_ok=True, repo_type="model")
            
            # å› ç‚ºæˆ‘å€‘åœ¨ç¬¬ 4 æ­¥å·²ç¶“æŠŠè³‡æ–™å¤¾æ•´ç†å¾—éå¸¸å®Œç¾ï¼Œæ‰€ä»¥ç›´æ¥ä¸Šå‚³æ•´å€‹è³‡æ–™å¤¾å³å¯ï¼
            print(f"Uploading the entire perfectly mixed folder...")
            api.upload_folder(
                folder_path=SAVE_PATH,
                repo_id=REPO_ID,
                repo_type="model",
                commit_message="Upload ablated weights with original configs via Representation Engineering"
            )
            
            print(f"\nğŸ‰ Success! Perfect model available at https://huggingface.co/{REPO_ID}")
            
        except Exception as e:
            print(f"Upload failed: {e}")
else:
    print("Skipping upload. Your perfect local model is ready.")
